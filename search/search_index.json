{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"optim\u012b","text":"<p>Fast, Modern, and Low Precision PyTorch Optimizers</p> <p>optimi enables accurate low precision training via Kahan summation, supports fully decoupled weight decay, and features fast implementations of modern optimizers.</p>"},{"location":"#low-precision-training-with-kahan-summation","title":"Low Precision Training with Kahan Summation","text":"<p>optimi optimizers can match the performance of mixed precision when training in BFloat16 by using Kahan summation.</p> <p>Training in BFloat16 with Kahan summation can reduce non-activation training memory usage by 37.5 to 45.5 percent when using an Adam optimizer. BFloat16 training increases single GPU training speed by ~10 percent at the same batch size.</p>"},{"location":"#fully-decoupled-weight-decay","title":"Fully Decoupled Weight Decay","text":"<p>In addition to supporting PyTorch-style decoupled weight decay, optimi optimizers also support fully decoupled weight decay.</p> <p>Fully decoupled weight decay decouples weight decay from the learning rate, more accurately following Decoupled Weight Decay Regularization. This can help simplify hyperparameter tuning as the optimal weight decay is no longer tied to the learning rate.</p>"},{"location":"#foreach-implementations","title":"Foreach Implementations","text":"<p>All optimi optimizers have fast foreach implementations, which can significantly outperform the for-loop versions. optimi reuses the gradient buffer for temporary variables to reduce foreach memory usage.</p>"},{"location":"#install","title":"Install","text":"<p>optimi is available to install from pypi.</p> <pre><code>pip install torch-optimi\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To use an optimi optimizer with Kahan summation and fully decoupled weight decay:</p> <pre><code>import torch\nfrom torch import nn\nfrom optimi import AdamW\n\n# create or cast model in low precision (bfloat16)\nmodel = nn.Linear(20, 1, dtype=torch.bfloat16)\n\n# instantiate AdamW with parameters and fully decoupled weight decay\n# Kahan summation is automatically enabled since model &amp; inputs are bfloat16\nopt = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5, decouple_lr=True)\n\n# forward and backward, casting input to bfloat16 if needed\nloss = model(torch.randn(20, dtype=torch.bfloat16))\nloss.backward()\n\n# optimizer step\nopt.step()\nopt.zero_grad()\n</code></pre> <p>To use with PyTorch-style weight decay with float32 or mixed precision:</p> <pre><code># create model\nmodel = nn.Linear(20, 1)\n\n# instantiate AdamW with parameters\nopt = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n</code></pre>"},{"location":"#differences-from-pytorch","title":"Differences from PyTorch","text":"<p>optimi optimizers do not support compilation, differentiation, complex numbers, or have capturable versions.</p> <p>optimi Adam optimizers do not support AMSGrad and SGD does not support Nesterov momentum. Optimizers which debias updates (Adam optimizers and Adan) calculate the debias term per parameter group, not per parameter.</p>"},{"location":"#optimizers","title":"Optimizers","text":"<p>optimi implements the following optimizers: Adam, AdamW, Adan, Lion, RAdam, Ranger, SGD, &amp; StableAdamW</p>"},{"location":"foreach/","title":"ForEach Optimizer Implementations","text":"<p>Like PyTorch, optimi supports foreach implementations of all optimizers. Foreach optimizers can be significantly faster than the for-loop versions.</p> <p>Foreach implementations can increase optimizer peak memory usage. optimi attempts to reduce this extra overhead by reusing the gradient buffer for temporary variables. If the gradients are required between the optimization step and gradient reset step, set <code>foreach=False</code> to use the for-loop implementation.</p> <p>If unspecified <code>foreach=None</code>, optimi will use the foreach implementation if training on a Cuda device.</p>"},{"location":"fully_decoupled_weight_decay/","title":"Fully Decoupled Weight Decay","text":"<p>In addition to supporting PyTorch-style decoupled weight decay, optimi optimizers also support fully decoupled weight decay.</p> <p>While PyTorch-style decoupled weight decay (hereafter referred to as \u201cdecoupled weight decay\u201d) decouples weight decay from the gradients, it explicitly couples weight decay with the learning rate \\(\\gamma_t\\lambda\\bm{\\theta}_{t-1}\\). This ties the optimal value of weight decay to the learning rate.</p> <p>In contrast, optimi\u2019s fully decoupled weight decay also decouples weight decay from the learning rate, more accurately following Decoupled Weight Decay Regularization by Loshchilov and Hutter.</p> <p>Fully decoupled weight decay is scaled by the relative learning rate \\((\\gamma_t/\\gamma_\\text{max})\\lambda\\bm{\\theta}_{t-1}\\) so applied weight decay will still follow the learning rate schedule.</p> Note: Implementation Inspired by Composer <p>optimi\u2019s fully decoupled weight decay implementation was inspired by Mosaic Composer\u2019s Decoupled Weight Decay.</p> <p>By default, optimi optimizers do not use fully decoupled weight decay for compatibility with existing PyTorch code.</p> <p>Enable fully decoupled weight decay by setting <code>decouple_lr=True</code> when initialing an optimi optimizer. If the initial learning rate <code>lr</code> isn\u2019t the maximum scheduled learning rate, pass it to <code>max_lr</code>.</p>"},{"location":"fully_decoupled_weight_decay/#hyperparameters","title":"Hyperparameters","text":"<p>Since fully decoupled weight decay is not multiplied by the learning rate each step, the optimal value for fully decoupled weight decay is smaller than decoupled weight decay.</p> <p>For example, to match AdamW\u2019s default decoupled weight decay of 0.01 with a maximum learning rate of \\(1\\times10^{-3}\\), set weight decay to \\(1\\times10^{-5}\\) when using fully decoupled weight decay.</p> <p>By default, optimi optimizers assume <code>lr</code> is the maximum scheduled learning rate. This allows the applied weight decay \\((\\gamma_t/\\gamma_\\text{max})\\lambda\\bm{\\theta}_{t-1}\\) to match the learning rate schedule. Set <code>max_lr</code> if this is not the case.</p>"},{"location":"fully_decoupled_weight_decay/#algorithm","title":"Algorithm","text":"<p>The algorithm below shows the difference between PyTorch\u2019s AdamW and optimi\u2019s Adam with fully decoupled weight decay.</p> \\[ \\begin{aligned}     &amp;\\rule{105mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#009ddb}{\\text{PyTorch\u2019s AdamW}} \\: \\text{\\&amp;} \\: \\textcolor{#9a3fe4}{\\text{Adam with fully decoupled weight decay}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\text{ (epsilon)};\\\\     &amp;\\hspace{17.25mm} \\gamma_\\text{max} \\: \\text{(maximum learning rate)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{105mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\textcolor{#009ddb}{+ \\lambda\\bm{\\theta}_{t-1}} \\bigr)\\textcolor{#9a3fe4}{- (\\gamma_t/\\gamma_\\text{max})\\lambda\\bm{\\theta}_{t-1}}\\\\[-0.5em]     &amp;\\rule{105mm}{0.4pt}\\\\ \\end{aligned} \\] <p>This difference applies to all optimi optimizers which implement both decoupled and fully decoupled weight decay.</p>"},{"location":"kahan_summation/","title":"Low Precision Training with Kahan Summation","text":"<p>While training models in low precision (Float16 or BFloat16) usually does not match training in full precision (Float32) or mixed precision, optimi optimizers match the performance of mixed precision when training in BFloat16 by using Kahan summation<sup>1</sup>.</p> <p>Training in low precision reduces memory usage and increases training speed relative to mixed precision training.</p> <p>Using Kahan summation for accurate BFloat16 training is as simple as replacing a PyTorch optimizer with its optimi equivalent and casting the model to BFloat16 instead of using mixed precision.</p> <p>Tip: Keep a Few Layers in Float32</p> <p>When training in BFloat16, keep rotary embedding layers in Float32 and consider keeping normalization layers in Float32, as these layers can benefit from full precision. This results in a small memory increase and speed decrease but can help guarantee equivalent results with mixed precision training.</p> <p>By default, optimi optimizers will automatically use Kahan summation for any layers training in low precision. Set <code>kahan_sum=False</code> to disable.</p>"},{"location":"kahan_summation/#mixed-precision","title":"Mixed Precision","text":"<p>While implementations details can differ, mixed precision works by running a forward pass in low precision, automatically switching to full precision per layer as needed, and then accumulating gradients during the backward pass in Float32. The optimizer step runs in full precision.</p> <p>The hybrid precision setup of mixed precision enables the faster operations and lower memory usage of low precision while keeping the convergence of full precision.</p>"},{"location":"kahan_summation/#kahan-summation","title":"Kahan Summation","text":"<p>Kahan summation<sup>2</sup> is a technique to reduce the numerical error of adding multiple low precision numbers by accumulating errors in a separate compensation buffer. The addition of the compensation buffer extends the effective summation precision by the precision of the compensation buffer.</p> <p>Using Kahan summation to improve low precision model training was first introduced by Zamirai et al in Revisiting BFloat16 Training. Zamirai et al discovered the primary source of numerical error from low precision training is during the optimizer\u2019s model weight update step. They add Kahan summation to the SGD &amp; AdamW weight update steps to reduce the update\u2019s numerical inaccuracy, increasing low precision training to the equivalent of full precision training across tested models.</p> Note: Implementation Inspired by TorchDistX <p>optimi\u2019s Kahan summation implementation was directly inspired by TorchDistX\u2019s <code>AnyPrecisionAdamW</code> optimizer.</p>"},{"location":"kahan_summation/#memory-savings","title":"Memory Savings","text":"<p>Training in BFloat16 with Kahan summation can reduce non-activation training memory usage by 37.5 to 45.5 percent when using an Adam optimizer, as Table 1 shows below.</p> <p>optimi reduces the potential extra memory overhead of Kahan summation by reusing the gradient buffer for temporary variables.</p> Table 1: Adam Per Parameter Memory Usage, Excluding Activations Buffer Mixed Precision BFloat16 + Kahan Sum BFloat16 BF16 Model Weights 2 bytes (if used) 2 bytes 2 bytes FP32 Model Weights 4 bytes - - Gradients 4 bytes 2 bytes 2 bytes Gradient Accumulation 4 bytes 2 bytes 2 bytes Momentum 4 bytes 2 bytes 2 bytes Variance 4 bytes 2 bytes 2 bytes Kahan Compensation - 2 bytes - Total 20-22 bytes 12 bytes 10 bytes <p>Calculating the total memory savings depends on activations and batch size, mixed precision implementation details, and the optimizer used, to name a few variables.</p>"},{"location":"kahan_summation/#training-speedup","title":"Training Speedup","text":"<p>Training in BFloat16 instead of mixed precision results in a ~10% speedup on a single GPU at the same batch size. BFloat16 training can further increase distributed training speed due to the halved bandwidth cost.</p>"},{"location":"kahan_summation/#algorithm","title":"Algorithm","text":"<p>SGD with Kahan summation.</p> \\[ \\begin{aligned}     &amp;\\rule{90mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#009ddb}{\\textbf{SGD}} \\: \\textcolor{#9a3fe4}{\\text{with Kahan summation}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)};\\\\     &amp;\\hspace{17.25mm} \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\: \\lambda \\: \\text{(weight decay)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\textcolor{#9a3fe4}{\\bm{c}_{0} \\leftarrow \\bm{0}}\\\\[-0.5em]     &amp;\\rule{90mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#009ddb}{\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t\\bm{g}_t}\\\\[0.3em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\bm{u} \\leftarrow \\bm{c}_{t-1} - \\gamma_t\\bm{g}_t}\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} + \\bm{u}}\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\bm{c}_t \\leftarrow \\bm{u} + (\\bm{\\theta}_{t-1} - \\bm{\\theta}_t)}\\\\[-0.5em]     &amp;\\rule{90mm}{0.4pt}\\\\ \\end{aligned} \\] <p>This shows the optimi implementation of Kahan summation optimizers, which is equivalent to the Revisiting BFloat16 Training formulation.</p> <ol> <li> <p>Current testing on small models shows no degradation in training performance.\u00a0\u21a9</p> </li> <li> <p>Also known as Kahan\u2013Babu\u0161ka summation or compensated summation.\u00a0\u21a9</p> </li> </ol>"},{"location":"which_optimizer/","title":"Which Optimizer Should I Use?","text":"<p>This guide is meant to provide a quick overview for choosing an optimi optimizer.</p> <p>All optimi optimizers support training in pure BFloat16 precision<sup>1</sup> using Kahan summation, which can help match Float32 optimizer performance with  mixed precision while reducing memory usage and increasing training speed.</p>"},{"location":"which_optimizer/#tried-and-true","title":"Tried and True","text":"<p>There\u2019s a reason AdamW is the default optimizer of deep learning. It performs well across multiple domains, model architectures, and batch sizes. Most optimizers claiming to outperform AdamW usually do not after careful analysis and experimentation.</p> <p>Consider reducing the \\(\\beta_2\\) term if training on large batch sizes or observing training loss spikes<sup>2</sup>.</p>"},{"location":"which_optimizer/#drop-in-replacement","title":"Drop-in Replacement","text":"<p>If using gradient clipping during training or experience training loss spikes, try replacing AdamW with StableAdamW. StableAdamW applies AdaFactor style update clipping to AdamW, stabilizing training loss and removing the need for gradient clipping.</p> <p>StableAdamW can outperform AdamW with gradient clipping on downstream tasks.</p>"},{"location":"which_optimizer/#low-memory-usage","title":"Low Memory Usage","text":"<p>If optimizer memory usage is important and optimi\u2019s Kahan summation doesn\u2019t alleviate optimizer memory usage or even more memory savings are desired, try optimi\u2019s two low memory optimizers: Lion and SGD.</p> <p>Lion uses one memory buffer for both momentum and the update step, reducing memory usage compared to AdamW. While reviews are mixed, Lion can match AdamW in some training scenarios.</p> <p>Prior to Adam and AdamW, SGD was the default optimizer for deep learning. SGD with Momentum can match or outperform AdamW on some tasks but can require more hyperparameter tuning. Consider using SGD with decoupled weight decay, it can lead to better results than L2 regularization.</p>"},{"location":"which_optimizer/#potential-upgrade","title":"Potential Upgrade","text":"<p>Adan can outperform AdamW at the expense of extra memory usage due to using two more buffers then AdamW. Consider trying Adan if optimizer memory usage isn\u2019t a priority, or when finetuning.</p>"},{"location":"which_optimizer/#small-batch-cnn","title":"Small Batch CNN","text":"<p>Ranger can outperform AdamW when training or finetuning on small batch sizes (~512 or less) with convolutional neural networks. It does use one more buffer then AdamW. Ranger performs best with a flat learning rate followed by a short learning rate decay.</p> <ol> <li> <p>Or BFloat16 with normalization and RoPE layers in Float32.\u00a0\u21a9</p> </li> <li> <p>This setting is mentioned in Sigmoid Loss for Language Image Pre-Training, although it is common knowledge in parts of the deep learning community.\u00a0\u21a9</p> </li> </ol>"},{"location":"optimizers/adam/","title":"Adam: Adaptive Moment Estimation","text":"<p>Adam (Adaptive Moment Estimation) computes per-parameter adaptive learning rates from the first and second gradient moments. Adam combines the advantages of two other optimizers: AdaGrad, which adapts the learning rate to the parameters, and RMSProp, which uses a moving average of squared gradients to set per-parameter learning rates. Adam also introduces bias-corrected estimates of the first and second gradient averages.</p> <p>Adam was introduced by Diederik Kingma and Jimmy Ba in Adam: A Method for Stochastic Optimization.</p>"},{"location":"optimizers/adam/#hyperparameters","title":"Hyperparameters","text":"<p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>If training on large batch sizes or observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of Adam combines Adam with both AdamW <code>decouple_wd=True</code> and Adam with fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/adam/#optimi.adam.Adam","title":"Adam","text":"<p>Adam optimizer. Optionally with decoupled weight decay (AdamW).</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/adam/#algorithm","title":"Algorithm","text":"<p>Adam with L2 regularization.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Adam}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s Adam also supports AdamW\u2019s decoupled weight decay and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/adamw/","title":"AdamW: Adam with Decoupled Weight Decay","text":"<p>AdamW improves upon Adam by decoupling weight decay from the gradients and instead applying weight decay directly to the model parameters. This modification allows AdamW to achieve better convergence and generalization than Adam.</p> <p>AdamW was introduced by Ilya Loshchilov and Frank Hutter in Decoupled Weight Decay Regularization.</p>"},{"location":"optimizers/adamw/#hyperparameters","title":"Hyperparameters","text":"<p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>If training on large batch sizes or observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of AdamW also supports fully decoupled weight decay <code>decouple_lr=True</code>. The default weight decay of 0.01 will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/adamw/#optimi.adamw.AdamW","title":"AdamW","text":"<p>AdamW optimizer: Adam with decoupled weight decay.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 1e-2)</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/adamw/#algorithm","title":"Algorithm","text":"<p>Adam with decoupled weight decay (AdamW).</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#009ddb}{\\textbf{Adam}} \\: \\textcolor{#9a3fe4}{\\text{with decoupled weigh decay (AdamW)}} \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) \\textcolor{#009ddb}{- \\lambda\\bm{\\theta}_{t-1}}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\textcolor{#9a3fe4}{+ \\lambda\\bm{\\theta}_{t-1}} \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s AdamW also supports fully decoupled weight decay, which is not shown.</p>"},{"location":"optimizers/adan/","title":"Adan: ADAptive Nesterov Momentum","text":"<p>Adan uses a efficient Nesterov momentum estimation method to avoid the extra computation and memory overhead of calculating the extrapolation point gradient. In contrast to other Nesterov momentum estimating optimizers, Adan estimates both the first- and second-order gradient movements. This estimation requires two additional buffers over AdamW, increasing memory usage.</p> <p>Adan was introduced by Xie et al in Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models.</p>"},{"location":"optimizers/adan/#hyperparameters","title":"Hyperparameters","text":"<p>Hyperparameter notes from Xie et al:</p> <ol> <li>\\(\\beta_2\\) is the least sensitive Adan hyperparameter, default of 0.92 works for majority of tasks</li> <li>Xie et al primarily tune \\(\\beta_3\\) (between 0.9-0.999) before \\(\\beta_1\\) (between 0.9-0.98) for different tasks</li> <li>Adan pairs well with large learning rates. Paper and GitHub report up to 3x larger than Lamb and up to 5-10x larger than AdamW</li> <li>Xie et al use the default weight decay of 0.02 for all tasks except fine-tuning BERT (0.01) and reinforcement learning (0)</li> </ol> <p>optimi\u2019s implementation of Adan also supports fully decoupled weight decay <code>decouple_lr=True</code>. The default weight decay of 0.02 will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p> Note: Adan in bfloat16 is Noisier then Other Optimizers <p>Even with Kahan summation, training with Adan in bfloat16 results in noisier updates relative to float32 or mixed precision training than other optimizers.</p>"},{"location":"optimizers/adan/#optimi.adan.Adan","title":"Adan","text":"<p>Adan Optimizer: Adaptive Nesterov Momentum Algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float, float]</code> <p>Coefficients for gradient, gradient difference, and squared gradient moving averages (default: (0.98, 0.92, 0.99))</p> <code>(0.98, 0.92, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 2e-2)</p> <code>0.02</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>adam_wd</code> <code>bool</code> <p>Apply weight decay before parameter update (Adam-style), instead of after the update per Adan algorithm (default: False)</p> <code>False</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>False</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/adan/#algorithm","title":"Algorithm","text":"<p>Adan: Adaptive Nesterov Momentum.</p> \\[ \\begin{align*}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Adan}  \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2, \\beta_3 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}; \\: \\bm{n}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) (\\bm{g}_t - \\bm{g}_{t-1})\\\\         &amp;\\hspace{10mm} \\bm{n}_t \\leftarrow \\beta_3 \\bm{n}_{t-1} + (1 - \\beta_3)\\bigl(\\bm{g}_t + \\beta_2(\\bm{g}_t - \\bm{g}_{t-1})\\bigr)^2\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{n}}_t \\leftarrow \\bm{n}_t/(1 - \\beta_3^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\eta}_t \\leftarrow \\gamma_t/(\\sqrt{\\hat{\\bm{n}}_t} + \\epsilon)\\\\         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow (1+\\gamma_t\\lambda )^{-1}\\bigl(\\bm{\\theta}_{t-1} - \\bm{\\eta}_t (\\hat{\\bm{m}}_t + \\beta_2\\hat{\\bm{v}}_t)\\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{align*} \\] <p>During the first step, \\(\\bm{g}_t - \\bm{g}_{t-1}\\) is set to \\(\\bm{0}\\).</p> <p>optimi\u2019s Adan also supports Adam-style weight decay and fully decoupled weight decay, both which are not shown.</p>"},{"location":"optimizers/lion/","title":"Lion: EvoLved Sign Momentum","text":"<p>Lion only keeps track of the gradient moving average (momentum) which can reduce memory usage compared to AdamW. Lion uses two momentum EMA factors, one for tracking momentum and another for using momentum in the update step. Using default hyperparameters, this allows up to ten times longer history for momentum tracking while leveraging more of the current gradient for the model update. Unlike most optimizers, Lion uses the same magnitude for each parameter update calculated using the sign operation.</p> <p>Lion was introduced by Chen et al in Symbolic Discovery of Optimization Algorithms.</p>"},{"location":"optimizers/lion/#hyperparameters","title":"Hyperparameters","text":"<p>Hyperparameter notes from Chen et al:</p> <ol> <li>Due to the larger update norm from the sign operation, a good Lion learning rate is typically 3-10X smaller than AdamW.</li> <li>Since the effective weight decay is multiplied by the learning rate<sup>1</sup>, weight decay should be increased by the learning rate decrease (3-10X).</li> <li>Except for language modeling, \\(\\beta\\)s are set to <code>(0.9, 0.99)</code>. When training T5, Chen at al set \\(\\beta_1=0.95\\) and \\(\\beta_2=0.98\\). Reducing \\(\\beta_2\\) results in better training stability due to less historical memorization.</li> <li>The optimal batch size for Lion is 4096 (vs AdamW\u2019s 256), but Lion still performs well at a batch size of 64 and matches or exceeds AdamW on all tested batch sizes.</li> </ol> <p>optimi\u2019s implementation of Lion also supports fully decoupled weight decay <code>decouple_lr=True</code>. If using fully decoupled weight decay do not increase weight decay. Rather, weight decay will likely need to be reduced as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/lion/#optimi.lion.Lion","title":"Lion","text":"<p>Lion optimizer. Evolved Sign Momentum.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for update moving average and gradient moving average (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 0)</p> <code>0</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/lion/#algorithm","title":"Algorithm","text":"<p>Lion: Evolved Sign Momentum.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Lion} \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{u} \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_2 \\bm{m}_{t-1} + (1 - \\beta_2) \\bm{g}_t\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\bigl(\\text{sign}(\\bm{u}) + \\lambda\\bm{\\theta}_{t-1} \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s Lion also supports fully decoupled weight decay, which is not shown.</p> <ol> <li> <p>The learning rate does not modify the effective weight decay when using fully decoupled weight decay.\u00a0\u21a9</p> </li> </ol>"},{"location":"optimizers/radam/","title":"RAdam: Rectified Adam","text":"<p>RAdam (Rectified Adam) is a variant of Adam which improves Adam\u2019s convergence by fixing the adaptive learning rate's large variance during early stages of training. RAdam estimates the variance of the squared gradient moving average and scales the update by this term to rectify the variance. RAdam is comparable to using a learning rate warmup schedule.</p> <p>RAdam was introduced by Liu et al in On the Variance of the Adaptive Learning Rate and Beyond.</p>"},{"location":"optimizers/radam/#hyperparameters","title":"Hyperparameters","text":"<p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>If training on large batch sizes or observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of RAdam supports both decoupled weight decay <code>decouple_wd=True</code> and fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/radam/#optimi.radam.RAdam","title":"RAdam","text":"<p>Rectified Adam optimizer. Optionally with decoupled weight decay.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: True)</p> <code>False</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/radam/#algorithm","title":"Algorithm","text":"<p>RAdam: Rectified Adam.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#9a3fe4}{\\textbf{Rectified}} \\: \\textbf{Adam}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)};\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}; \\: \\textcolor{#9a3fe4}{\\rho_{\\infty} \\leftarrow 2 / (1 - \\beta_2) - 1}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\rho_t \\leftarrow \\rho_{\\infty} - 2 t \\beta^t_2 /(1 - \\beta_2^t)}\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\textbf{if} \\: \\rho_t &gt; 5\\text{:}}\\\\         &amp;\\hspace{15mm} \\textcolor{#9a3fe4}{r_t \\leftarrow \\sqrt{\\tfrac{(\\rho_t - 4)(\\rho_t - 2)\\rho_{\\infty}}{(\\rho_{\\infty} - 4)(\\rho_{\\infty} -2 ) \\rho_t}}}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\textcolor{#9a3fe4}{r_t} \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\bigr)\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\textbf{else}\\text{:}}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t \\textcolor{#9a3fe4}{\\hat{\\bm{m}}_t}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s RAdam also supports decoupled weight decay and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/ranger/","title":"Ranger: RAdam and LookAhead","text":"<p>Ranger combines RAdam and Lookahead together in one optimizer. RAdam fixes the adaptive learning rate's large variance during early stages of training to improve convergence and reducing the need for warmup. Lookahead updates model weights like normal, but every k steps interpolates them with a copy of slow moving weights. This moving average of the model weights is less sensitive to suboptimal hyperparameters and reduces the need for hyperparameter tuning.</p> <p>Ranger was introduced by Less Wright in New Deep Learning Optimizer, Ranger: Synergistic combination of RAdam + Lookahead for the best of both.</p>"},{"location":"optimizers/ranger/#hyperparameters","title":"Hyperparameters","text":"<p>Ranger works best with a flat learning rate followed by a short learning rate decay. Try raising the learning rate 2-3x larger than AdamW.</p> <p>optimi sets the default \\(\\beta\\)s to <code>(0.9, 0.99)</code> and default \\(\\epsilon\\) to <code>1e-6</code>. These values reflect current best-practices and usually outperform the PyTorch defaults.</p> <p>optimi\u2019s implementation of Ranger supports both decoupled weight decay <code>decouple_wd=True</code> and fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/ranger/#optimi.ranger.Ranger","title":"Ranger","text":"<p>Ranger optimizer. RAdam with Lookahead.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>k</code> <code>int</code> <p>Lookahead synchronization period (default: 6)</p> <code>6</code> <code>alpha</code> <code>float</code> <p>Lookahead weight interpolation coefficient (default: 0.5)</p> <code>0.5</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: True)</p> <code>True</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/ranger/#algorithm","title":"Algorithm","text":"<p>Ranger: RAdam and LookAhead.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{Ranger: RAdam and} \\: \\textcolor{#9a3fe4}{\\textbf{LookAhead}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)};\\\\     &amp;\\hspace{17.25mm} \\bm{\\phi}_0 \\: \\text{(slow params)}; \\: k \\: \\text{(sync)}; \\: \\alpha \\: \\text{(interpolation)};\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\textcolor{#9a3fe4}{\\bm{\\phi}_0 \\leftarrow  \\bm{\\theta}_0}; \\: \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}; \\: \\rho_{\\infty} \\leftarrow 2 / (1 - \\beta_2) - 1;\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\rho_t \\leftarrow \\rho_{\\infty} - 2 t \\beta^t_2 /(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\textbf{if} \\: \\rho_t &gt; 5\\text{:}\\\\         &amp;\\hspace{15mm} r_t \\leftarrow \\sqrt{\\tfrac{(\\rho_t - 4)(\\rho_t - 2)\\rho_{\\infty}}{(\\rho_{\\infty} - 4)(\\rho_{\\infty} -2 ) \\rho_t}}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t r_t \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\bigr)\\\\         &amp;\\hspace{10mm} \\textbf{else}\\text{:}\\\\         &amp;\\hspace{15mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\gamma_t (\\hat{\\bm{m}}_t + \\lambda\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\textbf{if} \\: t \\equiv 0 \\pmod{k}\\text{:}}\\\\         &amp;\\hspace{15mm} \\textcolor{#9a3fe4}{\\bm{\\phi}_t \\leftarrow \\bm{\\phi}_{t-k} + \\alpha(\\bm{\\theta}_t - \\bm{\\phi}_{t-k} )}\\\\         &amp;\\hspace{15mm} \\textcolor{#9a3fe4}{\\bm{\\theta}_t  \\leftarrow \\bm{\\phi}_t}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>optimi\u2019s Ranger also supports L2 regularization and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/sgd/","title":"SGD: Stochastic Gradient Descent","text":"<p>Stochastic Gradient Descent iteratively updates the model parameters using the gradient from a mini-batch of data.</p> <p>SGD can be traced back to Herbert Robbins and Sutton Monro\u2019s stochastic approximation methods. Frank Rosenblatt was the first to use SGD to train neural networks in The perceptron: A probabilistic model for information storage and organization in the brain.</p>"},{"location":"optimizers/sgd/#hyperparmeters","title":"Hyperparmeters","text":"<p>SGD supports dampening <code>dampening=True</code>, where <code>dampening=1-momentum</code>. To match PyTorch\u2019s dampening set <code>torch_init=True</code>. This will initialize momentum buffer with first gradient instead of zeroes.</p> <p>optimi\u2019s implementation of SGD also supports decoupled weight decay <code>decouple_wd=True</code> and fully decoupled weight decay <code>decouple_lr=True</code>. Weight decay will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/sgd/#optimi.sgd.SGD","title":"SGD","text":"<p>SGD optimizer. Optionally with momentum and decoupled weight decay.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>momentum</code> <code>float</code> <p>Momentum factor. Gradient moving average coefficient if <code>dampening</code> is True (default: 0)</p> <code>0</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_wd</code> and <code>decouple_lr</code> are False, applies L2 penalty (default: 0)</p> <code>0</code> <code>dampening</code> <code>bool</code> <p>Use dampening for momentum update (default: False)</p> <code>False</code> <code>decouple_wd</code> <code>bool</code> <p>Apply decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of L2 penalty (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>torch_init</code> <code>bool</code> <p>Initialize momentum buffer with first gradient instead of zeroes. Enable to match PyTorch SGD when using dampening (default: False)</p> <code>False</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/sgd/#algorithm","title":"Algorithm","text":"<p>SGD with L2 regularization.</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textcolor{#dc3918}{\\textbf{SGD}} \\: \\textcolor{#009ddb}{\\text{with momentum}} \\: \\textcolor{#9a3fe4}{\\text{and dampening}}\\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta \\: \\text{(momentum)}; \\: \\lambda \\: \\text{(weight decay)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\textcolor{#009ddb}{\\bm{m}_{0} \\leftarrow \\bm{0}}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1}) - \\lambda\\bm{\\theta}_{t-1}\\\\         &amp;\\hspace{10mm} \\textcolor{#009ddb}{\\bm{m}_t \\leftarrow \\beta \\bm{m}_{t-1} +} \\textcolor{#9a3fe4}{(1 - \\beta)} \\textcolor{#009ddb}{\\bm{g}_t}\\\\         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} \\textcolor{#dc3918}{- \\gamma_t\\bm{g}_t} \\textcolor{#009ddb}{- \\gamma_t\\bm{m}_t}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>The SGD update terms \\(\\gamma_t\\bm{g}_t\\) and \\(\\gamma_t\\bm{m}_t\\) are exclusive, applying for SGD and SGD with momentum (and dampening), respectively. The dampening term \\((1 - \\beta)\\) is added to the momentum update \\(\\bm{m}_t \\leftarrow \\beta \\bm{m}_{t-1} + \\bm{g}_t\\) if enabled.</p> <p>optimi\u2019s SGD also supports AdamW\u2019s decoupled weight decay and fully decoupled weight decay, which are not shown.</p>"},{"location":"optimizers/stableadamw/","title":"StableAdamW: AdamW with Update Clipping","text":"<p>StableAdamW is a AdamW-Adafactor hybrid, porting Adafactor\u2019s update clipping into AdamW as a per parameter learning rate modification. StableAdamW\u2019s update clipping outperforms gradient clipping on downstream tasks while avoiding model training instability.</p> <p>StableAdamW was introduced by Wortsman et al in Stable and low-precision training for large-scale vision-language models.</p>"},{"location":"optimizers/stableadamw/#hyperparameters","title":"Hyperparameters","text":"<p>StableAdamW is a drop-in replacement for AdamW and uses the same hyperparameters, with one exception: StableAdamW removes the need for gradient clipping.</p> <p>If training on large batch sizes or still observing training loss spikes, consider reducing \\(\\beta_2\\) between \\([0.95, 0.99)\\).</p> <p>optimi\u2019s implementation of StableAdamW also supports fully decoupled weight decay <code>decouple_lr=True</code>. The default weight decay of 0.01 will likely need to be reduced when using fully decoupled weight decay as the learning rate will not modify the effective weight decay.</p>"},{"location":"optimizers/stableadamw/#optimi.stableadamw.StableAdamW","title":"StableAdamW","text":"<p>StableAdamW optimizer. An AdamW-Adafactor hybrid with learning rate update clipping.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterable[Tensor] | Iterable[dict]</code> <p>Iterable of parameters to optimize or dicts defining parameter groups</p> required <code>lr</code> <code>float</code> <p>Learning rate</p> required <code>betas</code> <code>tuple[float, float]</code> <p>Coefficients for gradient and squared gradient moving averages (default: (0.9, 0.99))</p> <code>(0.9, 0.99)</code> <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient. If <code>decouple_lr</code> is False, applies decoupled weight decay (default: 1e-2)</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Added to denominator to improve numerical stability (default: 1e-6)</p> <code>1e-06</code> <code>decouple_lr</code> <code>bool</code> <p>Apply fully decoupled weight decay instead of decoupled weight decay (default: False)</p> <code>False</code> <code>max_lr</code> <code>float | None</code> <p>Maximum scheduled learning rate. Set if <code>lr</code> is not the maximum scheduled learning rate and <code>decouple_lr</code> is True (default: None)</p> <code>None</code> <code>kahan_sum</code> <code>bool | None</code> <p>Enables Kahan summation for more accurate parameter updates when training in low precision (float16 or bfloat16). If unspecified, automatically applies for low precision parameters (default: None)</p> <code>None</code> <code>foreach</code> <code>bool | None</code> <p>Enables the foreach implementation. If unspecified, tries to use foreach over for-loop implementation since it is significantly faster (default: None)</p> <code>None</code>"},{"location":"optimizers/stableadamw/#algorithm","title":"Algorithm","text":"<p>StableAdam with decoupled weight decay (StableAdamW).</p> \\[ \\begin{aligned}     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{2mm} \\textbf{\\textcolor{#9a3fe4}{Stable}AdamW} \\\\     &amp;\\hspace{5mm} \\text{inputs} : \\bm{\\theta}_0 \\: \\text{(params)}; \\: f(\\bm{\\theta}) \\text{(objective)}; \\: \\gamma_t \\:\\text{(learning rate at } t \\text{)}; \\\\     &amp;\\hspace{17.25mm} \\beta_1, \\beta_2 \\: \\text{(betas)}; \\: \\lambda \\: \\text{(weight decay)}; \\: \\epsilon \\: \\text{(epsilon)}\\\\     &amp;\\hspace{5mm} \\text{initialize} : \\bm{m}_{0} \\leftarrow \\bm{0}; \\: \\bm{v}_{0} \\leftarrow \\bm{0}\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\     &amp;\\hspace{5mm} \\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}\\text{:}\\\\         &amp;\\hspace{10mm} \\bm{g}_t \\leftarrow \\nabla_{\\theta} f_t(\\bm{\\theta}_{t-1})\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t\\\\         &amp;\\hspace{10mm} \\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t\\\\[0.5em]         &amp;\\hspace{10mm} \\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t)\\\\         &amp;\\hspace{10mm} \\hat{\\bm{v}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\\\\[0.5em]         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\text{RMS}_t \\leftarrow  \\sqrt{\\mathbb{E[\\bm{g}^2_t/\\text{max}(\\bm{v}_t, \\epsilon^2)]}}}\\\\         &amp;\\hspace{10mm} \\textcolor{#9a3fe4}{\\eta_t \\leftarrow  \\gamma_t/\\text{max}(1,\\text{RMS}_t)}\\\\[0.5em]         &amp;\\hspace{10mm} \\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\textcolor{#9a3fe4}{\\eta_t} \\bigl( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\bigr)\\\\[-0.5em]     &amp;\\rule{100mm}{0.4pt}\\\\ \\end{aligned} \\] <p>Following Stable and low-precision training for large-scale vision-language models, the \\(\\text{RMS}_t\\) steps occur independantly for each tensor. Likewise, the \\(\\text{max}(\\bm{v}_t, \\epsilon^2)\\) term, instead of \\(\\sqrt{\\mathbb{E[\\bm{g}^2_t/\\bm{v}_t]}}\\), is added to prevent division by zero issues.</p> <p>optimi\u2019s StableAdamW also supports fully decoupled weight decay, which is not shown.</p>"}]}